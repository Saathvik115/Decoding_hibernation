# SRA & FASTQ

## Purpose
**SRA, also known as Sequence Read Archive**, is an NCBI repository containing high-sequence throughput files (usually short reads) generated by sequencing technologies like Illumina, 454, and IonTorrent. Due to the raw data these .sra files contain, they are often unusable until they are converted into a different format: a **FASTQ file**. These files include contains the sequence data from the clusters that pass filter on a flow cell and the quality score that associates with them. Each entry in a FASTQ files consists of 4 lines:

1. A sequence identifier with information about the sequencing run and the cluster. The exact contents of this line vary by based on the BCL to FASTQ conversion software used.

2. The sequence (the base calls; A, C, T, G and N).

3. A separator, which is simply a plus (+) sign.

4. The base call quality scores. These are Phred +33 encoded (Phred is a metric used to quantify the quality of these reads), using ASCII characters to represent the numerical quality scores.

Below is an example of a FASTQ file:
![fastqimage](https://github.com/user-attachments/assets/d9b9091e-3e48-431c-8cb1-7d03efcaaeff)

FASTQ files can contain up to millions of entries and can be several megabytes or gigabytes in size, which often makes them too large to open in a normal text editor. Generally, it is not necessary to view FASTQ files, because they are intermediate output files used as input for tools that perform downstream analysis (as done in this project), such as alignment to a reference or de novo assembly.

The bears in this project all have a corresponding SRA number generated from a PI after assigning their sequences to NCBI SRA. The next steps would be to then download the .sra files for each bear and convert each respective file to a .fastq counterpart.

## Methods
All steps in this bioinformatics pipeline mainly utilize a **Slurm script**. Slurm scripts (file_name.slurm) are used to submit and manage jobs in a high-performance computing (HPC) environment that uses the Slurm workload manager. Slurm is a popular open-source resource management and job scheduling application used on many HPC clusters and supercomputers (e.g. Hummingbird for UCSC) due to its ability to allocate/manage resources on HPC environments.
Below is an example of a Slurm script header at the top of a .slurm file:
```bash
#!/bin/bash
#SBATCH --job-name=my_job_name        # Job name
#SBATCH --output=output.txt           # Standard output file
#SBATCH --error=error.txt             # Standard error file
#SBATCH --partition=partition_name    # Partition or queue name
#SBATCH --nodes=1                     # Number of nodes
#SBATCH --ntasks-per-node=1           # Number of tasks per node
#SBATCH --cpus-per-task=1             # Number of CPU cores per task
#SBATCH --time=1:00:00                # Maximum runtime (D-HH:MM:SS)
#SBATCH --mail-type=END               # Send email at job completion
#SBATCH --mail-user=your@email.com    # Email address for notifications

#Load necessary modules (if needed)
#module load module_name

#Your job commands go here
#For example:
#python my_script.py

#Optionally, you can include cleanup commands here (e.g., after the job finishes)
#For example:
#rm some_temp_file.txt
```

In this step of the pipeline, we will be loading the *sratoolkit* module via our SLURM script. Additionally, using a SLURM script will allow us to use two tools derived from the sratoolkit package to utilize SRA and FASTQ files.
### `prefetch`
The `prefetch` tool in our `sratoolkit` module allows one to download the raw .sra files from NCBI, though they aren't readble yet. The syntax for `prefetch` in our Slurm script mirrors the following code:
```
prefetch sra# --output-directory desired/directory
```
`prefetch` mainly needs two parameters, the first being an SRA#  and the second being a desired path to a directory to produce an output .sra file in.
### `fasterq-dump`
The `fasterq-dump` tool in the `sratoolkit` module enables users to extract data in FASTQ or FASTA-format from SRA-accessions. To utilize  `fasterq-dump`, refer to the following code:
```
fasterq-dump path/to/sra/file --outdir desired/directory --threads n --temp temporary/directory --progress
```
The above code segment needs a path to the sra file that will be converted and a path to a directory to store the outputted .fastq files. Other parameters aren't necessary but will be covered in the following sections.

## Implementation
On my HPC (Hummingbird), I used the following Slurm script to run `prefetch` and `fasterq-dump`.
```bash
#!/bin/bash

#SBATCH --job-name=getSRA    			# Job name
#SBATCH --partition=128x24				# Partition name
#SBATCH --mail-type=ALL               		# Mail events (NONE, BEGIN, END, FAIL, ALL)
#SBATCH --mail-user=svoora@ucsc.edu   	# Where to send mail
#SBATCH --time=0-05:00:00 				# Wall clock time limit in Days-Hours:min:seconds
#SBATCH --ntasks=1                    		# Run a single task
#SBATCH --cpus-per-task=4                  	# Use 4 threads for fasterq-dump
#SBATCH --output=scripts/logs/fasterq-dump_%j.out    # Standard output and error log
#SBATCH --error=scripts/logs/fasterq-dump_%j.err     # Standard output and error log
#SBATCH --mem=8G                    		# Allocate memory for the job.
#SBATCH --array=1-11					# array job

# moves to working directory
cd /hb/groups/sip_eeb_01/saat || exit 1

#downloading any tools/modules
module load sratoolkit

#creates directory to store data
# and subdirectories for sra data, fastq files, and temp files
mkdir -p data/sra_data data/fastq-files data/tmp

#file with SRA info
SRA_FILE="data/bear_sex_season_sra.tsv"

#call line in file we're processing
LINE=$(sed -n "${SLURM_ARRAY_TASK_ID}p" "$SRA_FILE")
sra=$(echo ${LINE} | awk '{ print $4; }')

#print out which sample is being run
echo "downloading sra sample:${sra} ${sex} ${seasons} for bear: ${bear_name}"

#using prfetch to download SRA data
prefetch ${sra} --output-directory data/sra_data

echo "Downloading fastq files"

#using fasterq-dump to convert .sra to .fastq
fasterq-dump data/sra_data/${sra}/${sra}.sra --outdir data/fastq_files --threads 4 --temp data/tmp --progress
```
My Slurm header features two additional parameters, one for memory and another for an array job. Usually, for lower memory-intensive jobs like this one, it's okay to leave the SBATCH memory line out, but for jobs that need more procesisng power (as seen later on in this pipeline), it's necessary to specify the amount of memory one wants to allocate. I also use an array job for this Slurm script as instead of designing 11 different Slurm scripts each for a tailored SRA number, I used a column variable with all 11 SRA numbers and use an array to iterate through each one.

I loaded the `sratoolkit` module that the Slurm script needs in order to run `prefetch` and `fasterq-dump` and then made some directories to store all the .sra and .fastq files in. Depending on whether you want to make an array job or not, you'd need to store your data object (for me, it was a .tsv file) containing your SRA numbers and then extract the column with those SRA numbers. To store the data object, I made a variable called `SRA_FILE` containing my .tsv file with SRA numbers, and then used `sed` to create a variable called `LINE` that stores a particular line of my .tsv file, which will then be used in one of the eleven arrays in this job. I then intialized a variable called `sra` to the fourth column of the `LINE` variable, which stores the SRA numbers.

The echo line isn't necessary, but I found it to be helpful to as a debugging tool when I got an error in my output logs. I then used `prefetch` to download the .sra files using my `sra` variable from earlier via the syntax ${variable}; these files were then placed in my `sra_data` directory under the `data` folder. These .sra files were then converted into FASTQ files using fasterq-dump. I specified the number of processors I wanted to use by stating `--threads 4`, and I made a temporary directory for the data conversion to take place in via `--temp data/tmp`. Lastly, I used `--progress` to get a progress update on the conversion.
