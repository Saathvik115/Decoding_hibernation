# SRA & FASTQ

## Purpose
**SRA, also known as Sequence Read Archive**, is an NCBI repository containing high-sequence throughput files (usually short reads) generated by sequencing technologies like Illumina, 454, and IonTorrent. Due to the raw data these .sra files contain, they are often unusable until they are converted into a different format: a **FASTQ file**. These files include contains the sequence data from the clusters that pass filter on a flow cell and the quality score that associates with them. Each entry in a FASTQ files consists of 4 lines:

1. A sequence identifier with information about the sequencing run and the cluster. The exact contents of this line vary by based on the BCL to FASTQ conversion software used.

2. The sequence (the base calls; A, C, T, G and N).

3. A separator, which is simply a plus (+) sign.

4. The base call quality scores. These are Phred +33 encoded (Phred is a metric used to quantify the quality of these reads), using ASCII characters to represent the numerical quality scores.

Below is an example of a FASTQ file:
![fastqimage](https://github.com/user-attachments/assets/d9b9091e-3e48-431c-8cb1-7d03efcaaeff)

FASTQ files can contain up to millions of entries and can be several megabytes or gigabytes in size, which often makes them too large to open in a normal text editor. Generally, it is not necessary to view FASTQ files, because they are intermediate output files used as input for tools that perform downstream analysis (as done in this project), such as alignment to a reference or de novo assembly.

The bears in this project all have a corresponding SRA number generated from a PI after assigning their sequences to NCBI SRA. The next steps would be to then download the .sra files for each bear and convert each respective file to a .fastq counterpart.

## Methods
All steps in this bioinformatics pipeline mainly utilize a **Slurm script**. Slurm scripts (file_name.slurm) are used to submit and manage jobs in a high-performance computing (HPC) environment that uses the Slurm workload manager. Slurm is a popular open-source resource management and job scheduling application used on many HPC clusters and supercomputers (e.g. Hummingbird for UCSC) due to its ability to allocate/manage resources on HPC environments.
Below is an example of a Slurm script header at the top of a .slurm file:
```
#!/bin/bash
#SBATCH --job-name=my_job_name        # Job name
#SBATCH --output=output.txt           # Standard output file
#SBATCH --error=error.txt             # Standard error file
#SBATCH --partition=partition_name    # Partition or queue name
#SBATCH --nodes=1                     # Number of nodes
#SBATCH --ntasks-per-node=1           # Number of tasks per node
#SBATCH --cpus-per-task=1             # Number of CPU cores per task
#SBATCH --time=1:00:00                # Maximum runtime (D-HH:MM:SS)
#SBATCH --mail-type=END               # Send email at job completion
#SBATCH --mail-user=your@email.com    # Email address for notifications

#Load necessary modules (if needed)
#module load module_name

#Your job commands go here
#For example:
#python my_script.py

#Optionally, you can include cleanup commands here (e.g., after the job finishes)
#For example:
#rm some_temp_file.txt
```

In this step of the pipeline, we will be loading the *sratoolkit* module via our SLURM script. Additionally, using a SLURM script will allow us to use two tools derived from the sratoolkit package to utilize SRA and FASTQ files.
### `prefetch`
The `prefetch` tool in our `sratoolkit` module allows one to download the raw .sra files from NCBI, though they aren't readble yet. The syntax for `prefetch` in our Slurm script mirrors the following code:
```
prefetch sra# --output-directory desired/directory
```
`prefetch` mainly needs two parameters, the first being an SRA#  and the second being a desired path to a directory to produce an output .sra file in.
### `fasterq-dump`
The `fasterq-dump` tool in the `sratoolkit` module enables users to extract data in FASTQ or FASTA-format from SRA-accessions. To utilize  `fasterq-dump`, refer to the following code:
```
fasterq-dump path/to/sra/file --outdir desired/directory --threads n --temp temporary/directory --progress
```
The above code segment needs a path to the sra file that will be converted and a path to a directory to store the outputted .fastq files. Other parameters aren't necessary but will be covered in the following sections.

## Implementation
On my HPC (Hummingbird), I used the following Slurm script to run `prefetch` and `fasterq-dump`.
```
#!/bin/bash

#SBATCH --job-name=getSRA    			# Job name
#SBATCH --partition=128x24				# Partition name
#SBATCH --mail-type=ALL               		# Mail events (NONE, BEGIN, END, FAIL, ALL)
#SBATCH --mail-user=svoora@ucsc.edu   	# Where to send mail
#SBATCH --time=0-05:00:00 				# Wall clock time limit in Days-Hours:min:seconds
#SBATCH --ntasks=1                    		# Run a single task
#SBATCH --cpus-per-task=4                  	# Use 4 threads for fasterq-dump
#SBATCH --output=scripts/logs/fasterq-dump_%j.out    # Standard output and error log
#SBATCH --error=scripts/logs/fasterq-dump_%j.err     # Standard output and error log
#SBATCH --mem=8G                    		# Allocate memory for the job.
#SBATCH --array=1-11					# array job

# moves to working directory
cd /hb/groups/sip_eeb_01/saat || exit 1

#downloading any tools/modules
module load sratoolkit

#creates directory to store data
# and subdirectories for sra data, fastq files, and temp files
mkdir -p data/sra_data data/fastq-files data/tmp

#file with SRA info
SRA_FILE="data/bear_sex_season_sra.tsv"

#call line in file we're processing
LINE=$(sed -n "${SLURM_ARRAY_TASK_ID}p" "$SRA_FILE")
bear_name=$(echo ${LINE} | awk '{ print $1; }')
sex=$(echo ${LINE} | awk '{ print $2; }')
seasons=$(echo ${LINE} | awk '{ print $3; }')
sra=$(echo ${LINE} | awk '{ print $4; }')

#print out which sample is being run
echo "downloading sra sample:${sra} ${sex} ${seasons} for bear: ${bear_name}"

#using prfetch to download SRA data
prefetch ${sra} --output-directory data/sra_data

echo "Downloading fastq files"

#using fasterq-dump to convert .sra to .fastq
#The parameters used here are:
# --outdir: specifies the output directory for the fastq files
# --threads: specifies the number of threads to use for the conversion
# --temp: specifies the temporary directory to use during the conversion
# --progress: shows the progress of the conversion
fasterq-dump data/sra_data/${sra}/${sra}.sra --outdir data/fastq_files --threads 4 --temp data/tmp --progress
```

