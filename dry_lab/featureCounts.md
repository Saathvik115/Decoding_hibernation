# featureCounts

## Purpose

**featureCounts** is a highly efficient general-purpose read summarization program that counts mapped reads for genomic features such as genes, exons, promoters, gene bodies, genomic bins, and chromosomal locations. It is part of the **Subread package**, a comprehensive suite of software programs for processing next-generation sequencing read data. featureCounts serves as a critical quantification step in RNA-seq analysis pipelines, converting aligned reads (typically from tools like STAR) into gene expression count matrices that can be used for downstream differential expression analysis.

The primary purpose of featureCounts is to bridge the gap between read alignment and statistical analysis. After sequencing reads have been aligned to a reference genome, featureCounts systematically counts how many reads map to each genomic feature (typically genes) as defined in a GTF (Gene Transfer Format) annotation file. This quantification is essential because raw alignment files contain positional information about where each read maps, but differential expression analysis requires summarized count data representing the expression level of each gene.

featureCounts offers several advantages over other counting methods:

1. **High Performance**: Implements chromosome hashing, feature blocking, and other optimization strategies to assign reads to features with exceptional speed
2. **Flexibility**: Can count at both feature level (e.g., individual exons) and meta-feature level (e.g., genes), with support for various annotation formats
3. **Multi-threading Support**: Provides significant speed improvements on multi-core systems
4. **Paired-end Awareness**: Properly handles paired-end reads by counting fragments rather than individual reads when appropriate
5. **Comprehensive Output**: Generates both count matrices and detailed summary statistics

In this RNA-seq workflow, featureCounts processes the sorted BAM files generated by STAR alignment, using the brown bear (Ursus arctos) GTF annotation file that was used in STAR to quantify gene expression levels across all samples.

## Methods

All steps utilize **Slurm scripts** for execution on high-performance computing environments, as described in previous workflow steps. However, unlike the previous tools that used standard module loading, featureCounts requires a **conda environment** setup because it's not always available as a pre-installed module on all HPC systems.

### Conda Environment Management

**Conda** is a package manager that provides package, dependency, and environment management for any language. It creates isolated virtual environments that don't interfere with the system's existing software installations. **Bioconda** is a specialized conda channel that distributes bioinformatics software, making it the preferred method for installing tools like featureCounts.

The conda environment approach offers several advantages:
- **Dependency Management**: Automatically resolves and installs all required dependencies
- **Version Control**: Ensures reproducible analysis by using specific software versions
- **Isolation**: Prevents conflicts between different software versions
- **Portability**: Environments can be easily shared and recreated on different systems

### featureCounts Algorithm

featureCounts implements a sophisticated algorithm optimized for high-throughput data processing:

#### Chromosome Hashing
The algorithm begins by generating a hash table for reference sequence names, enabling rapid matching between reads in SAM/BAM files and features in the annotation file. This is particularly beneficial when dealing with large numbers of reference sequences.

#### Hierarchical Data Structure
After chromosome hashing, features are organized into a two-level hierarchy:
1. **Genomic Bins**: Reference sequences are divided into non-overlapping 128 kb bins
2. **Feature Blocks**: Within each bin, features are grouped into blocks based on their genomic positions

This hierarchical structure enables rapid read assignment by quickly narrowing down genomic regions that could contain features overlapping with query reads.

#### Read Assignment Logic
The algorithm processes reads through multiple decision points:
- **Feature vs. Meta-feature Level**: Can count reads for individual features (e.g., exons) or aggregate them for meta-features (e.g., genes)
- **Overlap Requirements**: Determines minimum overlap between reads and features required for assignment
- **Multi-mapping Handling**: Provides options for handling reads that map to multiple genomic locations
- **Paired-end Processing**: For paired-end data, counts fragments rather than individual reads for more accurate quantification

## Implementation

### Environment Setup

Before running featureCounts, I established a conda environment containing the necessary software. This process involves several steps that must be completed before submitting the Slurm job:

```bash
# First, load the miniconda3 module
module load miniconda3

# Create a new conda environment specifically for featureCounts
# The subread package contains featureCounts
conda create -n featureCounts_env -c bioconda subread

# Activate the conda environment
conda activate featureCounts_env
```

This setup process is crucial because:
-  `module load miniconda3` provides access to the conda package manager on the HPC system
-  `conda create -n featureCounts_env -c bioconda subread` creates an isolated environment named "featureCounts_env" and installs the subread package from the bioconda channel[22][24]
-  `conda activate featureCounts_env` activates the environment, making featureCounts available for use in subsequent commands

### Slurm Script Implementation

On my HPC (Hummingbird), I used the following Slurm script to run featureCounts for read quantification:

```bash
#!/bin/bash

#SBATCH --job-name=runFeatureCounts		# Job name
#SBATCH --partition=128x24				# Partition name
#SBATCH --mail-type=ALL               		# Mail events (NONE, BEGIN, END, FAIL, ALL)
#SBATCH --mail-user=svoora@ucsc.edu   	# Where to send mail
#SBATCH --time=0-10:00:00 				# Wall clock time limit in Days-Hours:min:seconds
#SBATCH --ntasks=1                 		# Run a single task
#SBATCH --cpus-per-task=8                	# Use 8 threads for featureCounts
#SBATCH --output=/hb/groups/sip_eeb_01/saat/scripts/logs/runfeaturecounts_%j.out    # Standard output and error log
#SBATCH --error=/hb/groups/sip_eeb_01/saat/scripts/logs/runfeaturecounts_%j.err     # Standard output and error log
#SBATCH --mem=8G                    # Allocate memory for the job.

# moves to working directory
cd /hb/groups/sip_eeb_01/saat || exit 1

#creates directory to store featureCounts data
mkdir -p analysis/3_featurecounts

#folder with fastq files
fastq="data/fastq_names.tsv"

#call line in file we're processing
LINE=$(sed -n "${SLURM_ARRAY_TASK_ID}p" "$fastq")
read1=$(echo ${LINE} | awk '{ print $1; }')

#running featureCounts
featureCounts -p -T 8 -t exon -g gene_id -a data/genome/GCF_023065955.2_UrsArc2.0_genomic.gtf -o analysis/3_featurecounts/bear_adipose_rawCounts.txt analysis/2_star/*_Aligned.sortedByCoord.out.bam
```

**Important Note**: While the script includes array job setup variables (`SLURM_ARRAY_TASK_ID`), the actual featureCounts command processes all BAM files simultaneously using a wildcard pattern (`analysis/2_star/*_Aligned.sortedByCoord.out.bam`). This is a more efficient approach for featureCounts, which can handle multiple input files in a single command. When I first ran it as an array job, I got individual featureCounts output files per SRA #, which isn't optimal for later analysis which requires all the outputs to be compiled within a single file.

### Slurm Header Analysis

The Slurm header includes several important parameters:
- **Memory Allocation (8G)**: While featureCounts is generally memory-efficient, 8GB ensures sufficient resources for processing multiple large BAM files simultaneously
- **CPU Allocation (8 threads)**: Matches the `-T 8` parameter in the featureCounts command for optimal parallel processing
- **Extended Runtime (10 hours)**: Allows adequate time for processing large BAM files, though actual runtime is typically much shorter

### featureCounts Command Breakdown

The main featureCounts command includes several critical parameters:

- **`-p`**: Specifies paired-end mode, counting fragments (paired reads) rather than individual reads. This is essential for paired-end data because it provides more accurate quantification - fragment counts cannot be obtained by simply dividing read counts by two due to various factors like read pair failures and different fragment sizes[3][6]

- **`-T 8`**: Sets the number of threads to 8, matching the CPU allocation in the Slurm header. Multi-threading significantly improves performance when processing large datasets[5][12]

- **`-t exon`**: Specifies the feature type to count. The default value "exon" means that reads mapping to exonic regions will be counted. Other options include "gene", "CDS", "start_codon", etc., depending on the GTF annotation content[1][5]

- **`-g gene_id`**: Specifies the attribute type for grouping features into meta-features. Using "gene_id" means that all exons belonging to the same gene will be aggregated to produce gene-level counts[1][5]

- **`-a data/genome/GCF_023065955.2_UrsArc2.0_genomic.gtf`**: Specifies the annotation file path. This GTF file contains the genomic coordinates and attributes for all genes and exons in the brown bear reference genome[5][9]

- **`-o analysis/3_featurecounts/bear_adipose_rawCounts.txt`**: Defines the output file path. featureCounts will create both the main count file and a summary file with ".summary" extension[5][9]

- **`analysis/2_star/*_Aligned.sortedByCoord.out.bam`**: Input BAM files from the STAR alignment step. The wildcard pattern includes all aligned BAM files, allowing featureCounts to process all samples in a single command[5]

### Output Files

featureCounts generates two primary output files:

1. **Count Matrix** (`bear_adipose_rawCounts.txt`): A tab-delimited file containing:
   - Gene information (Gene ID, chromosome, start/end positions, strand, length)
   - Read counts for each sample (one column per BAM file)
   - This file serves as input for downstream differential expression analysis

2. **Summary Statistics** (`bear_adipose_rawCounts.txt.summary`): Contains detailed information about:
   - Total number of reads processed
   - Successfully assigned reads
   - Reads that failed assignment due to various reasons (ambiguous, multi-mapping, etc.)
   - This file is crucial for quality control and troubleshooting

The resulting count matrix is ready for import into differential expression analysis tools such as DESeq2, edgeR, or limma, completing the transition from raw sequencing data to quantified gene expression levels suitable for biological interpretation and statistical analysis.
